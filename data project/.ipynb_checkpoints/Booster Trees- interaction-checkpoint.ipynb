{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 999\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureTrain = pd.read_csv('./data/dengue_features_train.csv',parse_dates = ['week_start_date'])\n",
    "TargetTrain = pd.read_csv('./data/dengue_labels_train.csv' )\n",
    "FeatureTest = pd.read_csv('./data/dengue_features_test.csv',parse_dates = ['week_start_date'])\n",
    "Answersheet = pd.read_csv('./data/submission_format.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at target first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is the difference between two measure\\n\\nhttps://journals.ametsoc.org/doi/pdf/10.1175/BAMS-D-14-00226.1\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the difference between two measure\n",
    "\n",
    "https://journals.ametsoc.org/doi/pdf/10.1175/BAMS-D-14-00226.1\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = TargetTrain.merge(FeatureTrain,on = ['city','year','weekofyear'] ,how = 'outer') \n",
    " \n",
    "kelvin = ['reanalysis_air_temp_k', 'reanalysis_avg_temp_k',   'reanalysis_max_air_temp_k','reanalysis_min_air_temp_k','reanalysis_dew_point_temp_k']\n",
    "panel.loc[:,kelvin] = panel.loc[:,kelvin]-273.15# kelvin to C\n",
    "FeatureTest.loc[:,kelvin] =FeatureTest.loc[:,kelvin]-273.15# kelvin to C\n",
    "panel.columns\n",
    "\n",
    "timeid = ['year', 'weekofyear']\n",
    "green = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw']\n",
    "precipitation  =['precipitation_amt_mm','reanalysis_sat_precip_amt_mm','station_precip_mm','reanalysis_precip_amt_kg_per_m2',]\n",
    "#precipitation = ['precipitation_amt_mm']\n",
    "avg_temp = ['reanalysis_air_temp_k', 'reanalysis_avg_temp_k',  'station_avg_temp_c' ]\n",
    "#avg_temp =  [   'station_avg_temp_c' ]     \n",
    "min_temp = ['station_min_temp_c','reanalysis_min_air_temp_k']\n",
    "#min_temp = ['station_min_temp_c' ]\n",
    "max_temp = ['station_max_temp_c','reanalysis_max_air_temp_k']\n",
    "#max_temp = ['station_max_temp_c']\n",
    "dtr =  ['reanalysis_tdtr_k', 'station_diur_temp_rng_c']\n",
    "#dtr =  [  'station_diur_temp_rng_c']\n",
    "\n",
    "humid = ['reanalysis_dew_point_temp_k','reanalysis_specific_humidity_g_per_kg', 'reanalysis_relative_humidity_percent']\n",
    "#humid = ['reanalysis_dew_point_temp_k','reanalysis_specific_humidity_g_per_kg']\n",
    " \n",
    "selected = humid + dtr + max_temp +min_temp +avg_temp +precipitation +timeid+green + ['city','total_cases','week_start_date']\n",
    "features_selected= humid + dtr + max_temp +min_temp +avg_temp +precipitation +timeid+green  \n",
    "# precipitation_amt_mm  & reanalysis_sat_precip_amt_mm are the same\n",
    " \n",
    "panel = panel.loc[:,selected]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj = panel.loc[panel.city == 'sj'].interpolate()  .set_index('week_start_date').copy().drop(['city'],axis = 1)\n",
    "iq = panel.loc[panel.city == 'iq']  .interpolate()  .set_index('week_start_date').copy().iloc[60:].drop(['city'],axis = 1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = 'iq'\n",
    "\n",
    "iq_test = FeatureTest.loc[FeatureTest.city == city_name].interpolate().set_index('week_start_date').copy().drop('city',axis = 1)\n",
    "iq_test['total_cases'] = np.nan \n",
    "\n",
    "iq = pd.concat([iq,iq_test ],sort = False)\n",
    " \n",
    "train_len = len(iq_test['total_cases'])\n",
    "\n",
    " \n",
    "data_selected = iq.loc[:,['total_cases']] .copy() \n",
    "data_candidate = iq .copy().iloc[: ]\n",
    "data_candidate['green_n'] = data_candidate.loc[:,green[:2]].mean(1)\n",
    "data_candidate['green_s'] = data_candidate.loc[:,green[2:]].mean(1)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrow = 0\\ncorr_limit = 0.95\\nwhile(row < length ):\\n    \\n    corrs = tmp.iloc[:int(len(tmp) ),:] .corr()\\n    col = row +1\\n    drops = []\\n    \\n    while(col < length ):\\n        if abs(corrs .iloc[row,col])>corr_limit:\\n            drops.append(tmp.columns[col])\\n        \\n        col += 1\\n    tmp .drop(drops,axis = 1,inplace = True)\\n    length = tmp.shape[1]\\n    row += 1\\ntmp = data_selected.loc[:,tmp.columns].dropna()\\nprint(tmp.shape)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "\n",
    "data_interaction = data_candidate.copy().drop(['weekofyear','year'],axis = 1)\n",
    "def append_shift(left,right,selected,shift,smooth =52):\n",
    "    new_names = []\n",
    "    for name in selected:\n",
    "        new_name = name + '_shift_' +str(shift)+'_smooth_' + str(smooth)\n",
    "        new_names.append(new_name)\n",
    " \n",
    "        left[new_name] = right.rolling(smooth  ).mean().shift(shift).loc[:,name ]\n",
    "    return left\n",
    "\n",
    "def append_diff(left,right,selected,diff,smooth =52):\n",
    "    new_names = []\n",
    "    for name in selected:\n",
    "        new_name = name + '_diff_' +str(diff)+'_smooth_' + str(smooth)\n",
    "        new_names.append(new_name)\n",
    " \n",
    "        left[new_name] = right.rolling(smooth  ).mean().diff(diff).loc[:,name ]\n",
    "    return left\n",
    "\n",
    " \n",
    "def seasonal_predict(data,params):\n",
    "    data = data.copy()\n",
    "    data[\"season_sin\"] = np.sin(data.index.weekofyear/53*6.2831)\n",
    "    data[\"season_cos\"] = np.cos(data.index.weekofyear/53*6.2831)\n",
    "    \n",
    "    expr = \"\"\"  ~ \"\"\"\n",
    "    expr = 'total_cases' + expr\n",
    "    \n",
    "    for name in [\"season_sin\",\"season_cos\"]:\n",
    "        expr +=     name  + ' + '\n",
    "    expr = expr[:-2]\n",
    "   \n",
    "    Y , X  = dmatrices(expr, data, return_type='dataframe')\n",
    "    \n",
    "    return  X.dot(params) \n",
    " \n",
    "\n",
    "def de_season_tri(data,target):\n",
    "    data = data.copy()\n",
    "    data[\"season_sin\"] = np.sin(data.index.weekofyear/53*6.2831)\n",
    "    data[\"season_cos\"] = np.cos(data.index.weekofyear/53*6.2831)\n",
    "    expr = \"\"\"  ~ \"\"\"\n",
    "    expr = target + expr\n",
    "    \n",
    "    for name in [\"season_sin\",\"season_cos\"]:\n",
    "        expr +=     name  + ' + '\n",
    "    expr = expr[:-2]\n",
    " \n",
    "    Y , X  = dmatrices(expr, data, return_type='dataframe')\n",
    "    if target == 'total_cases':\n",
    "        Y = Y.clip(0,50)\n",
    "    model = sm.OLS(Y,X)\n",
    "    results = model.fit()\n",
    "    \n",
    " \n",
    " \n",
    "    \n",
    "    return  data.loc[:,target] - results .predict(X)  ,results.params\n",
    "residuals,params = de_season_tri(data_candidate,'total_cases')\n",
    "data_selected['season_based'] = seasonal_predict(data_selected.fillna(0) ,params) \n",
    " \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "Interactions = PolynomialFeatures(degree = 2,include_bias = False)\n",
    "\n",
    "\n",
    "data_interaction_temp = data_interaction.drop('total_cases',axis = 1)\n",
    "interactions_val = Interactions .fit_transform(data_interaction_temp ) \n",
    "names = Interactions .get_feature_names(data_interaction_temp .columns)\n",
    "interactions = pd.DataFrame(interactions_val ,columns = names ,index =data_interaction_temp .index )\n",
    "\n",
    "interactions = interactions.rolling(20).mean().shift(2)\n",
    "interactions['total_cases'] = data_selected.total_cases\n",
    " \n",
    "data_selected = interactions\n",
    "\n",
    "\n",
    "def normalize(df, how ):\n",
    "    if how == 'z':\n",
    "        return (df - df.mean())/(df.std())\n",
    "    if how == 'r':\n",
    "        return (df - df.min())/(df.max() - df.min())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "data_selected_corr = data_selected .corr().total_cases.drop([ 'total_cases'])\n",
    "selected = data_selected_corr  .loc[abs(data_selected_corr)>0.15].sort_values().index\n",
    "tmp = data_selected.loc[:,selected ]\n",
    "length = tmp.shape[1]\n",
    "\"\"\"\n",
    "row = 0\n",
    "corr_limit = 0.95\n",
    "while(row < length ):\n",
    "    \n",
    "    corrs = tmp.iloc[:int(len(tmp) ),:] .corr()\n",
    "    col = row +1\n",
    "    drops = []\n",
    "    \n",
    "    while(col < length ):\n",
    "        if abs(corrs .iloc[row,col])>corr_limit:\n",
    "            drops.append(tmp.columns[col])\n",
    "        \n",
    "        col += 1\n",
    "    tmp .drop(drops,axis = 1,inplace = True)\n",
    "    length = tmp.shape[1]\n",
    "    row += 1\n",
    "tmp = data_selected.loc[:,tmp.columns].dropna()\n",
    "print(tmp.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected= data_selected.loc[:,['total_cases']].merge(tmp,\\\n",
    "                                                           left_index = True,right_index = True)\n",
    " \n",
    "data_selected.columns = [name.replace(' ','_') for name in data_selected.columns]\n",
    "data_selected = data_selected.fillna(0)\n",
    "data_selected['season_based'] = seasonal_predict(data_selected.fillna(0) ,params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(616, 160)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selected.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For IQ\n",
    "\n",
    "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.04, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
    "             random_state=1, reg_alpha=0, reg_lambda=10, scale_pos_weight=1,\n",
    "             seed=None, silent=None, subsample=1, verbosity=1)\n",
    "\n",
    "For SJ\n",
    "\n",
    "XGBRegressor(  base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.04, max_delta_step=0,\n",
    "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
    "             random_state=1, reg_alpha=0, reg_lambda=10, scale_pos_weight=1,\n",
    "             seed=None, silent=None, subsample=1, verbosity=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#data_temp = normalize( data_selected,'r')\n",
    "#data_temp['total_cases'] =  data_selected.total_cases\n",
    "\n",
    "#data_selected = data_temp.copy()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "data = data_selected.copy() .iloc[:-train_len ].dropna()\n",
    "data_test = data_selected.copy().iloc[-train_len: ].fillna(method = 'ffill') .fillna(method = 'bfill').replace(np.nan,0)\n",
    "kf = KFold(n_splits=4, random_state=None, shuffle=False)\n",
    "\n",
    "expr = \"\"\"total_cases ~   -1 + \"\"\"\n",
    "drops = ['total_cases'  ]\n",
    "for name in data.drop(drops,axis = 1).columns:\n",
    "    expr +=    name + ' + '\n",
    "expr=  expr[:-2]\n",
    "\n",
    "maes = []\n",
    "models = []\n",
    "test_xs = []\n",
    "test_ys = []\n",
    "pred_ys = []\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize(df, how ):\n",
    "    if how == 'z':\n",
    "        return (df - df.mean())/(df.std())\n",
    "    if how == 'r':\n",
    "        return (df - df.min())/(df.max() - df.min())\n",
    "\n",
    "\n",
    "    \n",
    "for train_index, test_index in kf.split(data):\n",
    "    \n",
    "    df_train = data.iloc[train_index,:]\n",
    "    df_test  = data.iloc[test_index,:]\n",
    "    \n",
    "    #train_y,  train_X = dmatrices(expr, df_train, return_type='dataframe')\n",
    "    train_y,train_X  = df_train.loc[:,['total_cases']],df_train.drop(['total_cases'],axis = 1)\n",
    "    #test_y,  test_X = dmatrices(expr, df_test, return_type='dataframe')\n",
    "    test_y,test_X  = df_test.loc[:,['total_cases']],df_test.drop(['total_cases'],axis = 1)\n",
    "    test_xs.append(test_X)\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    train_y = train_y.iloc[:,0] .clip(0,150).rolling(3,center = True).mean() .dropna()\n",
    "    train_X = train_X.loc[train_y.index]\n",
    "    \"\"\"model\"\"\"\n",
    "    model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=1,\n",
    "             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n",
    "             max_depth=2, min_child_weight=2, missing=None, n_estimators=400,\n",
    "             n_jobs=4, nthread=None, objective='reg:squarederror',\n",
    "             random_state=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "             seed=None, silent=None, subsample=1, verbosity=1)\n",
    "    \n",
    "    \n",
    "    model .fit(X=train_X, y=pd.DataFrame(train_y))\n",
    "    pred_res = pd.Series(model.predict(test_X)  ,index =  test_y .index)\n",
    "\n",
    "    pred_res = pd.Series(pred_res,index =df_test.index )\n",
    "    \n",
    "    \n",
    "    pred_ys.append(pred_res)\n",
    "    test_ys.append(test_y)\n",
    "    \n",
    "    models.append(model)\n",
    "    maes .append(abs(test_y.values.ravel()- pred_res.values.ravel()).mean())\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat(test_ys) \n",
    "validate =pd.DataFrame( pd.concat(pred_ys)) \n",
    "\n",
    "ensamble_samples = validate.merge(test,left_index= True,right_index = True )\n",
    "ensamble_samples.to_csv('./data/ensamble_xgb_' +str(city_name) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.197260879433673, 10.357853977576546, 9.398311119494231, 8.759959931995558]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1  = pd.Series(models[0].feature_importances_,index= train_X.columns).sort_values().iloc[-100:]\n",
    "m2  = pd.Series(models[1].feature_importances_,index= train_X.columns).sort_values().iloc[-100:] \n",
    "m3  = pd.Series(models[2].feature_importances_,index= train_X.columns).sort_values().iloc[-100:]\n",
    "m4  = pd.Series(models[3].feature_importances_,index= train_X.columns).sort_values().iloc[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=1,\n",
       "             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=2, missing=None, n_estimators=400,\n",
       "             n_jobs=4, nthread=None, objective='reg:squarederror',\n",
       "             random_state=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total  = data \n",
    "    \n",
    "total_y,  total_X = df_total .loc[:,['total_cases']],df_total .drop(['total_cases'],axis = 1)\n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=1,\n",
    "             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n",
    "             max_depth=2, min_child_weight=2, missing=None, n_estimators=400,\n",
    "             n_jobs=4, nthread=None, objective='reg:squarederror',\n",
    "             random_state=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "             seed=None, silent=None, subsample=1, verbosity=1)\n",
    "model.fit( total_X ,total_y.iloc[:,[0]].clip(0,150))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "submit_X = data_test  .drop(['total_cases'],axis = 1)\n",
    "\n",
    "pd.Series(np.round(model.predict(submit_X)).astype(int),index =  submit_X .index).\\\n",
    "to_csv(\"./data/0127_xgb_\" + str(city_name) + \".csv\",header = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "parameters = {'booster': ['gbtree'],'random_state':[1] , 'learning_rate':[0.04,0.06,0.1],\\\n",
    "              'n_estimators' :[100,200,300,400,500] , 'objective' : [  'reg:squarederror'] ,\\\n",
    "                               'reg_lambda' :[ 1],'gamma' : [10,50,100,200,300]}\n",
    "model =  XGBRegressor()\n",
    "clf = GridSearchCV(model, parameters)\n",
    "total_y =  total_y.iloc[:,[0]].clip(0,150).rolling(3).mean().dropna()\n",
    "total_X = total_X.loc[total_y .index]\n",
    "clf.fit(total_X ,total_y )\n",
    "\n",
    "sorted(clf.cv_results_.keys())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a74ad04ea648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
