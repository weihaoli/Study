{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.options.display.max_rows = 999\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureTrain = pd.read_csv('./data/dengue_features_train.csv',parse_dates = ['week_start_date'])\n",
    "TargetTrain = pd.read_csv('./data/dengue_labels_train.csv' )\n",
    "FeatureTest = pd.read_csv('./data/dengue_features_test.csv',parse_dates = ['week_start_date'])\n",
    "Answersheet = pd.read_csv('./data/submission_format.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                  Presentation Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = TargetTrain.merge(FeatureTrain,on = ['city','year','weekofyear'] ,how = 'outer') \n",
    " \n",
    "kelvin = ['reanalysis_air_temp_k', 'reanalysis_avg_temp_k',   \\\n",
    "          'reanalysis_max_air_temp_k','reanalysis_min_air_temp_k','reanalysis_dew_point_temp_k']\n",
    "panel.loc[:,kelvin] = panel.loc[:,kelvin]-273.15# kelvin to C\n",
    "FeatureTest.loc[:,kelvin] =FeatureTest.loc[:,kelvin]-273.15# kelvin to C\n",
    "panel.columns\n",
    "timeid = ['year', 'weekofyear']\n",
    "green = ['ndvi_ne', 'ndvi_nw', 'ndvi_se', 'ndvi_sw']\n",
    "precipitation  =['precipitation_amt_mm','reanalysis_sat_precip_amt_mm','station_precip_mm','reanalysis_precip_amt_kg_per_m2',]\n",
    "avg_temp = ['reanalysis_air_temp_k', 'reanalysis_avg_temp_k',  'station_avg_temp_c' ]\n",
    "min_temp = ['station_min_temp_c','reanalysis_min_air_temp_k']\n",
    "max_temp = ['station_max_temp_c','reanalysis_max_air_temp_k']\n",
    "dtr =  ['reanalysis_tdtr_k', 'station_diur_temp_rng_c']\n",
    "humid = ['reanalysis_dew_point_temp_k','reanalysis_specific_humidity_g_per_kg', 'reanalysis_relative_humidity_percent']\n",
    "selected = humid + dtr + max_temp +min_temp +avg_temp +precipitation +timeid+green + ['city','total_cases','week_start_date']\n",
    "features_selected= humid + dtr + max_temp +min_temp +avg_temp +precipitation +timeid+green  \n",
    "panel = panel.loc[:,selected]\n",
    "\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['reanalysis_dew_point_temp_k','reanalysis_max_air_temp_k','reanalysis_min_air_temp_k',\\\n",
    "           'green_s','green_n','precipitation_amt_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def get_data_panel(panel,city_name):\n",
    "    panel_train =  panel.loc[panel.city == city_name].interpolate()  .set_index('week_start_date').copy().drop(['city'],axis = 1).copy()\n",
    "    \n",
    "    panel_test = FeatureTest.loc[FeatureTest.city == city_name].interpolate().set_index('week_start_date').copy().drop('city',axis = 1)\n",
    "    panel_test['total_cases'] = np.nan \n",
    "    train_len = len(panel_test['total_cases'])\n",
    "    panel = pd.concat([panel_train,panel_test],sort = False)\n",
    "    \n",
    "    \n",
    "    panel['green_s']  = panel.loc[:,green[:2]].mean(1)\n",
    "    panel['green_n']  = panel.loc[:,green[2:]].mean(1) \n",
    "    panel['green']  = panel.loc[:,green ].mean(1) \n",
    "\n",
    "    panel  = panel \n",
    "    return panel,train_len\n",
    "\n",
    "panel_sj,len_sj = get_data_panel(panel,'sj')\n",
    "panel_iq,len_iq = get_data_panel(panel,'iq')\n",
    "\n",
    "def min_max(data):\n",
    "     \n",
    "    return (data - data.min())/(data .max()- data.min())\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "\n",
    "drops = ['reanalysis_specific_humidity_g_per_kg']  + green +\\\n",
    "['reanalysis_sat_precip_amt_mm','reanalysis_avg_temp_k','year','weekofyear','total_cases'] \n",
    "\n",
    "def append_shift(left,right,selected,shift,smooth =52):\n",
    "    new_names = []\n",
    "    right = right.copy().apply(lambda x : x.clip(x.quantile(0.05),x.quantile(0.95)),axis = 0)\n",
    "    for name in selected:\n",
    "        new_name = name + '_shift_' +str(shift)+'_smooth_' + str(smooth)\n",
    "        new_names.append(new_name)\n",
    " \n",
    "        left[new_name] = right.rolling(smooth     ).mean().shift(shift).loc[:,name ]\n",
    "    return left\n",
    "\n",
    "def append_diff(left,right,selected,diff,smooth =52):\n",
    "    new_names = []\n",
    "    right = right.copy().apply(lambda x : x.clip(x.quantile(0.05),x.quantile(0.95)),axis = 0)\n",
    "    for name in selected:\n",
    "        new_name = name + '_diff_'  +'_smooth_' + str(smooth)\n",
    "        new_names.append(new_name)\n",
    " \n",
    "        left[new_name] = right.diff(1).rolling(smooth   ).mean().loc[:,name ]\n",
    "    return left\n",
    " \n",
    "def de_season_tri(series):\n",
    "    \n",
    "    target = series.name\n",
    "    series = pd.DataFrame(series.copy())\n",
    "    series[\"season_sin\"] = np.sin(series.index.weekofyear/53*6.2831)\n",
    "    series[\"season_cos\"] = np.cos(series.index.weekofyear/53*6.2831)\n",
    "\n",
    "\n",
    "    X_out = series.drop(target,axis = 1) \n",
    "    X_out  = sm.add_constant(X_out) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_df = series.dropna()\n",
    "    train_y = train_df  .loc[:,target]\n",
    "    train_y =train_y.clip(train_y.quantile(0.05),train_y.quantile(0.95))\n",
    "    train_X = train_df  .drop(target,axis = 1)\n",
    "    train_X = sm.add_constant(train_X)\n",
    "    model = sm.OLS(train_y,train_X)\n",
    "    res = model.fit()\n",
    "    return  series.loc[:,target] .ravel()/res  .predict(X_out) .ravel() - 1\n",
    "\n",
    "\n",
    "def get_season_tri(series):\n",
    "    \n",
    "    target = series.name\n",
    "    series = pd.DataFrame(series.copy())\n",
    "    series[\"season_sin\"] = np.sin(series.index.weekofyear/53*6.2831)\n",
    "    series[\"season_cos\"] = np.cos(series.index.weekofyear/53*6.2831)\n",
    "\n",
    "\n",
    "    X_out = series.drop(target,axis = 1) \n",
    "    X_out  = sm.add_constant(X_out) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_df = series.dropna()\n",
    "    train_y = train_df  .loc[:,target]\n",
    "    train_y =train_y.clip(train_y.quantile(0.05),train_y.quantile(0.95))\n",
    "    train_X = train_df  .drop(target,axis = 1)\n",
    "    train_X = sm.add_constant(train_X)\n",
    "    model = sm.OLS(train_y,train_X)\n",
    "    res = model.fit()\n",
    "    return   res  .predict(X_out) .ravel()  \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "panel_sj,len_sj = get_data_panel(panel,'sj')\n",
    " \n",
    "panel_iq,len_iq = get_data_panel(panel,'iq')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_class:\n",
    "    \"\"\"\n",
    "    dataset iterator \n",
    "    \"\"\"\n",
    "    def __init__(self,inputs,targets):\n",
    "        self.inputs = inputs \n",
    "        self.targets = targets \n",
    "        self.limit   = len(targets)\n",
    "        \n",
    "        \n",
    "    def __iter__(self): \n",
    "        self.counter = 0 \n",
    "        return self\n",
    "    \n",
    "    def __next__(self): \n",
    "  \n",
    "        # Store current value ofx \n",
    "        self.counter  \n",
    "  \n",
    "   \n",
    "        if self.counter >= self.limit: \n",
    "            raise StopIteration \n",
    "        x = self.counter\n",
    "        # Else increment and return old value \n",
    "        \n",
    "        self.counter  = self.counter + 1 \n",
    "        return self.inputs[x],self.targets[x] \n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "def make_samples(train_X,train_y,size):\n",
    "    Xs,Ys = [],[]\n",
    "    \n",
    "    while(len(Xs) <size):\n",
    "        start = np.random.randint(52)\n",
    "        while(start < len(train_X)):\n",
    "            if start + 52 < len(train_X):\n",
    "                Xs.append(train_X.iloc[start:start + 52].values)\n",
    "                Ys.append(train_y.iloc[start:start + 52].values)\n",
    "            else:\n",
    "                break\n",
    "            start += 52\n",
    "    return Xs,Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\interview_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "panel_sj_selected  = panel_sj.loc[:,['total_cases']]\n",
    "panel_iq_selected  = panel_iq.loc[:,['total_cases']] \n",
    "np.random.seed(10)\n",
    "def data_process_random_forest(panel,panel_candidate ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make sure it is at least forward filled\n",
    "    \"\"\"\n",
    "    panel = append_shift(panel,panel_candidate,\\\n",
    "          panel_candidate.columns,0,52)\n",
    "    \n",
    "    panel = append_shift(panel,panel_candidate,\\\n",
    "          panel_candidate.columns,0,20)\n",
    "    \n",
    "    panel = append_diff(panel,panel_candidate,\\\n",
    "          panel_candidate.columns,26,26)\n",
    "    \n",
    "    \n",
    "    # de - seasons\n",
    "    de_seasons = panel_candidate.apply(de_season_tri,axis = 0)\n",
    "    new_names = [name +'_ds' for name in de_seasons.columns]\n",
    "    de_seasons.columns = new_names\n",
    "    \n",
    "    \n",
    "    panel = append_shift(panel,de_seasons,\\\n",
    "          de_seasons.columns,0,52)\n",
    "    \n",
    "    panel = append_shift(panel,de_seasons,\\\n",
    "          de_seasons.columns,0,10)\n",
    "    \n",
    "    panel = append_diff(panel,de_seasons,\\\n",
    "          de_seasons.columns,26,30)\n",
    " \n",
    "    return panel .fillna(method = 'ffill')\n",
    "\n",
    "panel_sj_selected =  data_process_random_forest(panel_sj_selected,panel_sj.drop(drops,axis = 1))\n",
    "panel_iq_selected =  data_process_random_forest(panel_iq_selected,panel_iq.drop(drops,axis = 1)) \n",
    "#panel_sj_selected['season'] = get_season_tri(panel_sj_selected.total_cases )\n",
    "#panel_iq_selected['season'] = get_season_tri(panel_iq_selected.total_cases )\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['station_min_temp_c_ds_shift_0_smooth_10',\n",
    "       'reanalysis_precip_amt_kg_per_m2_ds_shift_0_smooth_52',\n",
    "       'reanalysis_tdtr_k_ds_shift_0_smooth_52', 'green_ds_diff__smooth_30',\n",
    "       'green_diff__smooth_26', 'green_n_diff__smooth_26',\n",
    "       'green_n_ds_diff__smooth_30', 'green_s_ds_diff__smooth_30',\n",
    "       'reanalysis_precip_amt_kg_per_m2_ds_diff__smooth_30',\n",
    "       'station_precip_mm_ds_diff__smooth_30',\n",
    "       'precipitation_amt_mm_ds_diff__smooth_30',\n",
    "       'reanalysis_air_temp_k_ds_diff__smooth_30',\n",
    "       'reanalysis_min_air_temp_k_ds_diff__smooth_30',\n",
    "       'station_diur_temp_rng_c_ds_diff__smooth_30',\n",
    "       'reanalysis_relative_humidity_percent_ds_diff__smooth_30',\n",
    "       'green_s_diff__smooth_26', 'station_precip_mm_diff__smooth_26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def normalize(df, how ):\n",
    "    if how == 'z':\n",
    "        return (df - df.mean())/(df.std())\n",
    "    if how == 'r':\n",
    "        return (df - df.min())/(df.max() - df.min())\n",
    "    \n",
    "    \n",
    "    \n",
    "train_y_sj, train_X_sj =   panel_sj_selected  .iloc[: -len_sj].dropna().loc[:,'total_cases'],\\\n",
    "                             normalize(panel_sj_selected,'z' ).iloc[: -len_sj] .dropna() .loc[:,selected]\n",
    " \n",
    "train_y_iq, train_X_iq =   panel_iq_selected .iloc[60: -len_iq].dropna() .loc[:,'total_cases'],\\\n",
    "                             normalize(panel_iq_selected,'z').iloc[60: -len_iq] .dropna() .loc[:,selected]\n",
    "                         \n",
    " \n",
    "\n",
    "test_X_sj = panel_sj_selected  .iloc[-len_sj: ]\n",
    "test_X_iq = panel_iq_selected   .iloc[-len_iq: ]\n",
    "\n",
    "train_Xs,train_Ys =  make_samples(train_X_sj.iloc[ :500,1:].loc[:,selected].dropna(axis = 1),train_y_sj.iloc[:500 ],40)\n",
    "validate_Xs,validate_Ys =  make_samples(train_X_sj.iloc[500: ,1:].loc[:,selected].dropna(axis = 1),train_y_sj.iloc[500:  ],10)\n",
    "training_set = dataset_class(train_Xs,train_Ys)\n",
    "validation_set = dataset_class(validate_Xs,validate_Ys ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lstm): LSTM(16, 5)\n",
      "  (l_out): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "feature_size = len(selected)-1\n",
    "hidden_size = 5\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        # YOUR CODE HERE!\n",
    "        self.lstm = nn.LSTM(input_size=feature_size,\n",
    "                         hidden_size=hidden_size,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=hidden_size,\n",
    "                            out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "num_epochs = 200\n",
    "net = Net()\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "training_loss, validation_loss = [], []\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=3e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(num_epochs ) :\n",
    "\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "\n",
    "    net.eval()\n",
    "    for inputs, targets in validation_set:\n",
    "        #  validating paries\n",
    "        inputs = torch.Tensor(np.array([inputs,]) ).permute(1,0,2)\n",
    "        targets = torch.LongTensor(targets)\n",
    "        \n",
    "        outputs = net.forward(inputs)\n",
    "        loss = criterion(outputs, targets)    \n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "   \n",
    "\n",
    "    net.train()\n",
    "    for inputs, targets in training_set:\n",
    "\n",
    "        inputs = torch.Tensor(np.array([inputs ]) ).permute(1,0,2)\n",
    "        targets = torch.LongTensor(targets) .view(len(targets),1)\n",
    "        outputs = net.forward(inputs)\n",
    "        loss = criterion(outputs, targets) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "     \n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # early stop \n",
    "    if epoch % 5 == 0 and  validation_loss[-1] > validation_loss[-5]:\n",
    "        break\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
