{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "We have 100 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The word corresponding to index 1 is 'b'\n",
      "Our one-hot encoding of 'a' has shape (4,).\n",
      "Our one-hot encoding of 'a b' has shape (2, 4, 1) .\n"
     ]
    }
   ],
   "source": [
    "# Set seed such that we always get the same dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=100):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 10)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0]) \n",
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "  \n",
    "    # A bit of Python-magic to flatten a nested list\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: num_words)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        # YOUR CODE HERE!\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', word_to_idx['b'])\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\n",
    "\n",
    "\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    " \n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape} .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 80 samples in the training set.\n",
      "We have 10 samples in the validation set.\n",
      "We have 10 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "class dataset_class:\n",
    "    \"\"\"\n",
    "    iterator \n",
    "    \"\"\"\n",
    "    def __init__(self,inputs,targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.limit   = len(targets)\n",
    "        \n",
    "        \n",
    "    def __iter__(self): \n",
    "        self.counter = 0 \n",
    "        return self\n",
    "    \n",
    "    def __next__(self): \n",
    "  \n",
    "        # Store current value ofx \n",
    "       \n",
    "        \n",
    "        \n",
    "        self.counter  \n",
    "  \n",
    "   \n",
    "        if self.counter >= self.limit: \n",
    "            raise StopIteration \n",
    "        x = self.counter\n",
    "        # Else increment and return old value \n",
    "        \n",
    "        self.counter  = self.counter + 1 \n",
    "        return self.inputs[x],self.targets[x] \n",
    "    \n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "        \n",
    "\n",
    "\n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences,  dataset_class)\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(0,1,(3,1))\n",
    "def non_linear_y(x):\n",
    "    return int((np.sin(x[0]*2*3.14)*x[1] + x[2] ) *2)\n",
    "\n",
    "non_linear_y(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start implementation of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,derivative = False):\n",
    "    \"\"\"\n",
    "    Sigmid\n",
    "    \n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "         # if derititive it is a function from y to dy\n",
    "        return x*(1 - x)\n",
    "    return np.exp(x) /(1 +np.exp(x) )\n",
    "\n",
    "def tanh(x,derivative = False):\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        return 1 - x*x\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax\n",
    "    \"\"\"\n",
    "    return np.exp(x)/np.sum( np.exp(x))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init_lstm(X_size,H_size,Z_size ):\n",
    "    \"\"\"\n",
    "    initialize parameters for lstm\n",
    "    W_f , b_f   - forget gate\n",
    "    W_i,  b_i   - input gate\n",
    "    W_c,  b_c   - candidates\n",
    "    W_o,  b_o   - output gate \n",
    "    \n",
    "    \n",
    "    W_v,b_v   \n",
    "    \"\"\"\n",
    "    W_f = np.random.randn(H_size,Z_size)\n",
    "    b_f = np.ones((H_size,1))\n",
    "    \n",
    "    W_i = np.random.randn(H_size,Z_size)\n",
    "    b_i = np.zeros((H_size,1))\n",
    "    \n",
    "    W_c = np.random.randn(H_size,Z_size)\n",
    "    b_c = np.zeros((H_size,1))\n",
    "    \n",
    "    W_o =  np.random.randn(H_size,Z_size)\n",
    "    b_o =  np.zeros((H_size,1))\n",
    "    \n",
    "    \n",
    "    ## Parts related to out put function - to be modified\n",
    "    \n",
    "    W_v = np.random.randn(X_size,H_size)\n",
    "    b_v = np.zeros((X_size,1))\n",
    "    \n",
    "    return W_f,b_f,W_i,b_i,W_c,b_c,W_o,b_o,W_v,b_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test forwardation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params,x,h_prev,c_prev):\n",
    "    \"\"\"\n",
    "    forward porpergation - given input return output\n",
    "    \n",
    "    \"\"\"\n",
    "    W_f,b_f,W_i,b_i,W_g,b_g,W_o,b_o,W_v,b_v = params\n",
    " \n",
    "    z = np.concatenate([h_prev,x])\n",
    " \n",
    "    # fortegt gate\n",
    "    f = sigmoid (W_f.dot(z) + b_f)\n",
    "    # input gate\n",
    "    i = sigmoid (W_i.dot(z) + b_i) \n",
    "    # candidates\n",
    "    g = tanh(W_g.dot(z) + b_g)  \n",
    "    # update c\n",
    "    c = c_prev*f + i*g\n",
    "    #out put\n",
    "    o = sigmoid (W_o.dot(z) + b_o)\n",
    "    # update hidden state\n",
    "    h = o*tanh(c)\n",
    "    ## Parts related to out put function - to be modified\n",
    "    v = W_v.dot(h) + b_v\n",
    "    ## calculate softmax\n",
    "    output = softmax(v)\n",
    "    return z,f,i,g,c,o,h,v,output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(parameters,d_params,target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "               ):\n",
    "    \n",
    "    W_f,b_f,W_i,b_i,W_C,b_C,W_o,b_o,W_v,b_v  = params\n",
    "    d_W_f,d_b_f,d_W_i,d_b_i,d_W_C,d_b_C,d_W_o,d_b_o,d_W_v,d_b_v = d_params\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    d_W_v += np.dot(dv, h.T)\n",
    "    d_b_v += dv\n",
    "\n",
    "    dh = np.dot(W_v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    d_W_o += np.dot(do, z.T)\n",
    "    d_b_o += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    d_W_C += np.dot(dC_bar, z.T)\n",
    "    d_b_C += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    d_W_i += np.dot(di, z.T)\n",
    "    d_b_i += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    d_W_f += np.dot(df, z.T)\n",
    "    d_b_f += df\n",
    "\n",
    "    dz = (np.dot(W_f.T, df)\n",
    "         + np.dot(W_i.T, di)\n",
    "         + np.dot(W_C.T, dC_bar)\n",
    "         + np.dot(W_o.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def backward_pass(params,d_params,target ,z ,f,i ,g ,c ,o ,h ,v ,output, d_h_next,d_c_next,C_prev):\n",
    "    \"\"\"\n",
    "    backpropagation\n",
    "    \"\"\"\n",
    "    W_f,b_f,W_i,b_i,W_g,b_g,W_o,b_o,W_v,b_v  = params\n",
    "    d_W_f,d_b_f,d_W_i,d_b_i,d_W_g,d_b_g,d_W_o,d_b_o,d_W_v,d_b_v = d_params\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    # now evaluate loss and take derivitive \n",
    "    d_v =np.copy( output)\n",
    "    d_v[target.astype(bool)]-= 1\n",
    "\n",
    "\n",
    "\n",
    "    #update weight in output function\n",
    "    d_W_v +=  (d_v)*(h.T)\n",
    "    d_b_v += d_v\n",
    "\n",
    "    #from output layer to lstm output h\n",
    "    d_h  =W_v.T.dot(d_v) \n",
    "    d_h += d_h_next#(dL_t/dh_t + dL_t+1 /dh_t )\n",
    "\n",
    "    # from h to output gate\n",
    "    d_o = d_h * tanh(c)\n",
    "    # from output gate to z  \n",
    "    d_o = d_o* sigmoid(o,True)  \n",
    "    d_W_o  += d_o*(z.T)  \n",
    "    d_b_o  += d_o\n",
    "    # from output(tanh) gate to C\n",
    "\n",
    "    d_c = np.copy(d_c_next)\n",
    "    d_c += d_h * o*tanh(tanh(c),True)\n",
    "    # from C to gain g\n",
    "    d_g = d_c*i\n",
    "    d_g *= tanh(g,True)\n",
    "    # now we can calculate derivitive in g\n",
    "    d_W_g += d_g *(z.T)\n",
    "    d_b_g += d_g \n",
    "    # now wrt i\n",
    "    d_i = d_c*g\n",
    "    d_i = d_i*sigmoid(i,True)\n",
    "    d_W_i += d_i *z.T\n",
    "    d_b_i += d_i\n",
    "\n",
    "    # now wrt forget gate\n",
    "    d_f = C_prev*d_c\n",
    "    d_f = d_f*sigmoid(f,True)\n",
    "    d_W_f += d_f *z.T\n",
    "    d_b_f += d_f\n",
    "\n",
    "\n",
    "\n",
    "    d_z = W_f.T.dot(d_f) + W_i.T.dot(d_i) + W_g.T.dot(d_g) + W_o.T.dot(d_o)\n",
    "    d_h_prev = d_z[:len(d_h_next)]\n",
    "    d_c_prev = f*d_c\n",
    "\n",
    "    return d_h_prev,d_c_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test backwardation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_backward(inputs,targets,params,h_prev,c_prev):\n",
    "    x_s,z_s,f_s,i_s = {},{},{},{}\n",
    "    g_s,c_s,o_s,h_s = {},{},{},{}\n",
    "    v_s,y_s         = {},{} \n",
    "\n",
    "    h_s[-1],c_s[-1] = h_prev,c_prev\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = inputs[t]\n",
    "        z_s[t],f_s[t],i_s[t] ,g_s[t],c_s[t],o_s[t],h_s[t],v_s[t],y_s[t] = \\\n",
    "        forward_pass(params,x_s[t],h_s[t-1],c_s[t-1])\n",
    "        \n",
    "        loss += -np.mean(targets[t] *(np.log(y_s[t]+ 1e-12 ) ) )\n",
    "\n",
    "\n",
    "    d_params = ([np.zeros(param.shape) for param in params])\n",
    "    d_h_next,d_c_next = np.zeros(h_prev.shape),np.zeros(c_prev.shape)\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "\n",
    "        target = targets[t]\n",
    "        d_h_next,d_c_next  =  backward_pass(params,d_params,target = target ,z = z_s[t] ,f = f_s[t],\\\n",
    "                      i = i_s[t] ,g = g_s[t] ,c = c_s[t] ,o = o_s[t] ,h = h_s[t] ,v = v_s[t] ,\\\n",
    "                      output = y_s[t], d_h_next = d_h_next,d_c_next = d_c_next,C_prev = c_s[t-1])\n",
    "        \n",
    "    return loss, d_params,h_s[len(h_s) - 2], c_s[len(c_s) - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params,d_params,learning_rate = 1e-3):\n",
    " \n",
    "    for param, grad in zip(params, d_params):\n",
    "        param -= learning_rate  * grad\n",
    "       \n",
    "    return params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "X_size,H_size = len(word_to_idx),50\n",
    "Z_size = X_size + H_size\n",
    "\n",
    "params = init_lstm(X_size,H_size,Z_size)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    base_loss = 0 \n",
    "    \n",
    "    for train_X, train_y in training_set:\n",
    "        h_prev,c_prev = np.zeros((H_size,1)) , np.zeros((H_size,1))\n",
    "        inputs =  one_hot_encode_sequence(train_X,4)\n",
    "        targets = one_hot_encode_sequence(train_y,4)\n",
    "        \n",
    "        loss, d_params,h_prev,c_prev = forward_backward(inputs,targets,params,h_prev,c_prev)\n",
    "        d_params = clip_gradient_norm(d_params )\n",
    "        \n",
    "        update_params(params,d_params,learning_rate = 1e-1)\n",
    " \n",
    "        base_loss += loss\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    losses.append(base_loss/len(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5884756289549169\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1970deb7cc0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "print(min(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a095d306a341>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-a095d306a341>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def predict()\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
